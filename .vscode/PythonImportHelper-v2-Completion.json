[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "get_asgi_application",
        "importPath": "django.core.asgi",
        "description": "django.core.asgi",
        "isExtraImport": true,
        "detail": "django.core.asgi",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "HandDetector",
        "importPath": "cvzone.HandTrackingModule",
        "description": "cvzone.HandTrackingModule",
        "isExtraImport": true,
        "detail": "cvzone.HandTrackingModule",
        "documentation": {}
    },
    {
        "label": "HandDetector",
        "importPath": "cvzone.HandTrackingModule",
        "description": "cvzone.HandTrackingModule",
        "isExtraImport": true,
        "detail": "cvzone.HandTrackingModule",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "Classifier",
        "importPath": "cvzone.ClassificationModule",
        "description": "cvzone.ClassificationModule",
        "isExtraImport": true,
        "detail": "cvzone.ClassificationModule",
        "documentation": {}
    },
    {
        "label": "pyttsx3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyttsx3",
        "description": "pyttsx3",
        "detail": "pyttsx3",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "admin",
        "importPath": "django.contrib",
        "description": "django.contrib",
        "isExtraImport": true,
        "detail": "django.contrib",
        "documentation": {}
    },
    {
        "label": "path",
        "importPath": "django.urls",
        "description": "django.urls",
        "isExtraImport": true,
        "detail": "django.urls",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "JsonResponse",
        "importPath": "django.http",
        "description": "django.http",
        "isExtraImport": true,
        "detail": "django.http",
        "documentation": {}
    },
    {
        "label": "render",
        "importPath": "django.shortcuts",
        "description": "django.shortcuts",
        "isExtraImport": true,
        "detail": "django.shortcuts",
        "documentation": {}
    },
    {
        "label": "redirect",
        "importPath": "django.shortcuts",
        "description": "django.shortcuts",
        "isExtraImport": true,
        "detail": "django.shortcuts",
        "documentation": {}
    },
    {
        "label": "UserCreationForm",
        "importPath": "django.contrib.auth.forms",
        "description": "django.contrib.auth.forms",
        "isExtraImport": true,
        "detail": "django.contrib.auth.forms",
        "documentation": {}
    },
    {
        "label": "AuthenticationForm",
        "importPath": "django.contrib.auth.forms",
        "description": "django.contrib.auth.forms",
        "isExtraImport": true,
        "detail": "django.contrib.auth.forms",
        "documentation": {}
    },
    {
        "label": "login",
        "importPath": "django.contrib.auth",
        "description": "django.contrib.auth",
        "isExtraImport": true,
        "detail": "django.contrib.auth",
        "documentation": {}
    },
    {
        "label": "logout",
        "importPath": "django.contrib.auth",
        "description": "django.contrib.auth",
        "isExtraImport": true,
        "detail": "django.contrib.auth",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "WordNetLemmatizer",
        "importPath": "nltk.stem",
        "description": "nltk.stem",
        "isExtraImport": true,
        "detail": "nltk.stem",
        "documentation": {}
    },
    {
        "label": "TextBlob",
        "importPath": "textblob",
        "description": "textblob",
        "isExtraImport": true,
        "detail": "textblob",
        "documentation": {}
    },
    {
        "label": "nltk",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nltk",
        "description": "nltk",
        "detail": "nltk",
        "documentation": {}
    },
    {
        "label": "finders",
        "importPath": "django.contrib.staticfiles",
        "description": "django.contrib.staticfiles",
        "isExtraImport": true,
        "detail": "django.contrib.staticfiles",
        "documentation": {}
    },
    {
        "label": "login_required",
        "importPath": "django.contrib.auth.decorators",
        "description": "django.contrib.auth.decorators",
        "isExtraImport": true,
        "detail": "django.contrib.auth.decorators",
        "documentation": {}
    },
    {
        "label": "get_wsgi_application",
        "importPath": "django.core.wsgi",
        "description": "django.core.wsgi",
        "isExtraImport": true,
        "detail": "django.core.wsgi",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "application",
        "kind": 5,
        "importPath": "A2SL.asgi",
        "description": "A2SL.asgi",
        "peekOfCode": "application = get_asgi_application()",
        "detail": "A2SL.asgi",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "A2SL.datacollect",
        "description": "A2SL.datacollect",
        "peekOfCode": "cap = cv2.VideoCapture(0)\ndetector = HandDetector(maxHands=2)\noffset = 20\nimgSize = 128\ncounter = 0\nfolder = \"ThankYou\"\nwhile True:\n    success, img = cap.read()\n    hands, img = detector.findHands(img)\n    if hands:",
        "detail": "A2SL.datacollect",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "A2SL.datacollect",
        "description": "A2SL.datacollect",
        "peekOfCode": "detector = HandDetector(maxHands=2)\noffset = 20\nimgSize = 128\ncounter = 0\nfolder = \"ThankYou\"\nwhile True:\n    success, img = cap.read()\n    hands, img = detector.findHands(img)\n    if hands:\n        hand = hands[0]",
        "detail": "A2SL.datacollect",
        "documentation": {}
    },
    {
        "label": "offset",
        "kind": 5,
        "importPath": "A2SL.datacollect",
        "description": "A2SL.datacollect",
        "peekOfCode": "offset = 20\nimgSize = 128\ncounter = 0\nfolder = \"ThankYou\"\nwhile True:\n    success, img = cap.read()\n    hands, img = detector.findHands(img)\n    if hands:\n        hand = hands[0]\n        x, y, w, h = hand['bbox']",
        "detail": "A2SL.datacollect",
        "documentation": {}
    },
    {
        "label": "imgSize",
        "kind": 5,
        "importPath": "A2SL.datacollect",
        "description": "A2SL.datacollect",
        "peekOfCode": "imgSize = 128\ncounter = 0\nfolder = \"ThankYou\"\nwhile True:\n    success, img = cap.read()\n    hands, img = detector.findHands(img)\n    if hands:\n        hand = hands[0]\n        x, y, w, h = hand['bbox']\n        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255",
        "detail": "A2SL.datacollect",
        "documentation": {}
    },
    {
        "label": "counter",
        "kind": 5,
        "importPath": "A2SL.datacollect",
        "description": "A2SL.datacollect",
        "peekOfCode": "counter = 0\nfolder = \"ThankYou\"\nwhile True:\n    success, img = cap.read()\n    hands, img = detector.findHands(img)\n    if hands:\n        hand = hands[0]\n        x, y, w, h = hand['bbox']\n        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255\n        imgCrop = img[y-offset:y + h + offset, x-offset:x + w + offset]",
        "detail": "A2SL.datacollect",
        "documentation": {}
    },
    {
        "label": "folder",
        "kind": 5,
        "importPath": "A2SL.datacollect",
        "description": "A2SL.datacollect",
        "peekOfCode": "folder = \"ThankYou\"\nwhile True:\n    success, img = cap.read()\n    hands, img = detector.findHands(img)\n    if hands:\n        hand = hands[0]\n        x, y, w, h = hand['bbox']\n        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8)*255\n        imgCrop = img[y-offset:y + h + offset, x-offset:x + w + offset]\n        imgCropShape = imgCrop.shape",
        "detail": "A2SL.datacollect",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nSECRET_KEY = '3k7=!d39#4@_&5a6to&4=_=j(c^v0(vv91cj5+9e8+d4&+01jb'\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\nALLOWED_HOSTS = []\n# Application definition\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "SECRET_KEY",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "SECRET_KEY = '3k7=!d39#4@_&5a6to&4=_=j(c^v0(vv91cj5+9e8+d4&+01jb'\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\nALLOWED_HOSTS = []\n# Application definition\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "DEBUG",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "DEBUG = True\nALLOWED_HOSTS = []\n# Application definition\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "ALLOWED_HOSTS",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "ALLOWED_HOSTS = []\n# Application definition\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "INSTALLED_APPS",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "INSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n]\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "MIDDLEWARE",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "MIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\nROOT_URLCONF = 'A2SL.urls'",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "ROOT_URLCONF",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "ROOT_URLCONF = 'A2SL.urls'\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': ['templates',],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "TEMPLATES",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "TEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': ['templates',],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "WSGI_APPLICATION",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "WSGI_APPLICATION = 'A2SL.wsgi.application'\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n# Password validation\nAUTH_PASSWORD_VALIDATORS = [\n    {",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "DATABASES",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "DATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n# Password validation\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "AUTH_PASSWORD_VALIDATORS",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "AUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "LANGUAGE_CODE",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "LANGUAGE_CODE = 'en-us'\nTIME_ZONE = 'UTC'\nUSE_I18N = True\nUSE_L10N = True\nUSE_TZ = True\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "TIME_ZONE",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "TIME_ZONE = 'UTC'\nUSE_I18N = True\nUSE_L10N = True\nUSE_TZ = True\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),\n    os.path.join(BASE_DIR,\"Model\"),\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "USE_I18N",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "USE_I18N = True\nUSE_L10N = True\nUSE_TZ = True\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),\n    os.path.join(BASE_DIR,\"Model\"),\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "USE_L10N",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "USE_L10N = True\nUSE_TZ = True\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),\n    os.path.join(BASE_DIR,\"Model\"),\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "USE_TZ",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "USE_TZ = True\nSTATIC_URL = '/static/'\nSTATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),\n    os.path.join(BASE_DIR,\"Model\"),\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "STATIC_URL",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "STATIC_URL = '/static/'\nSTATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),\n    os.path.join(BASE_DIR,\"Model\"),\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "STATICFILES_DIRS",
        "kind": 5,
        "importPath": "A2SL.settings",
        "description": "A2SL.settings",
        "peekOfCode": "STATICFILES_DIRS = [    \n    os.path.join(BASE_DIR,\"assets\"),\n    os.path.join(BASE_DIR,\"Data\"),\n    os.path.join(BASE_DIR,\"Model\"),\n]",
        "detail": "A2SL.settings",
        "documentation": {}
    },
    {
        "label": "speak_label",
        "kind": 2,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "def speak_label(label):\n    engine.say(label)                                          \n    engine.runAndWait()\ncap = cv2.VideoCapture(0)   \ndetector = HandDetector(maxHands=2)\nclassifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\noffset = 20\nimgSize = 128\ncounter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "cap",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "cap = cv2.VideoCapture(0)   \ndetector = HandDetector(maxHands=2)\nclassifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\noffset = 20\nimgSize = 128\ncounter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "detector",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "detector = HandDetector(maxHands=2)\nclassifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\noffset = 20\nimgSize = 128\ncounter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "classifier",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "classifier = Classifier(\"Model/keras_model.h5\", \"Model/labels.txt\")\noffset = 20\nimgSize = 128\ncounter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "offset",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "offset = 20\nimgSize = 128\ncounter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "imgSize",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "imgSize = 128\ncounter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window\noutput_width = int(screen_width * 1.2)",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "counter",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "counter = 0\nlabels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window\noutput_width = int(screen_width * 1.2)\noutput_height = int(screen_height * 1.2)",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "labels = [\"Hello\", \"I Love you\", \"No\", \"okay\", \"Thank you\", \"yes\"]\n# Initialize pyttsx3 engine\nengine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window\noutput_width = int(screen_width * 1.2)\noutput_height = int(screen_height * 1.2)\n# Define the dirty orange color in BGR format",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "engine = pyttsx3.init()\n# Get screen resolution\nscreen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window\noutput_width = int(screen_width * 1.2)\noutput_height = int(screen_height * 1.2)\n# Define the dirty orange color in BGR format\ndirty_orange = (0, 166, 255)  # #ffa600 in BGR\nquit_flag = False",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "screen_width",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "screen_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nscreen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window\noutput_width = int(screen_width * 1.2)\noutput_height = int(screen_height * 1.2)\n# Define the dirty orange color in BGR format\ndirty_orange = (0, 166, 255)  # #ffa600 in BGR\nquit_flag = False\nwhile not quit_flag:\n    success, img = cap.read()",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "screen_height",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "screen_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n# Decrease the size of the output window\noutput_width = int(screen_width * 1.2)\noutput_height = int(screen_height * 1.2)\n# Define the dirty orange color in BGR format\ndirty_orange = (0, 166, 255)  # #ffa600 in BGR\nquit_flag = False\nwhile not quit_flag:\n    success, img = cap.read()\n    if not success:",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "output_width",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "output_width = int(screen_width * 1.2)\noutput_height = int(screen_height * 1.2)\n# Define the dirty orange color in BGR format\ndirty_orange = (0, 166, 255)  # #ffa600 in BGR\nquit_flag = False\nwhile not quit_flag:\n    success, img = cap.read()\n    if not success:\n        print(\"Failed to read frame.\")\n        continue  # Continue to the next iteration of the loop if frame reading fails",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "output_height",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "output_height = int(screen_height * 1.2)\n# Define the dirty orange color in BGR format\ndirty_orange = (0, 166, 255)  # #ffa600 in BGR\nquit_flag = False\nwhile not quit_flag:\n    success, img = cap.read()\n    if not success:\n        print(\"Failed to read frame.\")\n        continue  # Continue to the next iteration of the loop if frame reading fails\n    imgOutput = np.zeros((screen_height, screen_width, 3), np.uint8)  # Create black background",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "dirty_orange",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "dirty_orange = (0, 166, 255)  # #ffa600 in BGR\nquit_flag = False\nwhile not quit_flag:\n    success, img = cap.read()\n    if not success:\n        print(\"Failed to read frame.\")\n        continue  # Continue to the next iteration of the loop if frame reading fails\n    imgOutput = np.zeros((screen_height, screen_width, 3), np.uint8)  # Create black background\n    imgOutput[0:screen_height, 0:screen_width] = img  # Place original frame on black background\n    hands, img = detector.findHands(img)",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "quit_flag",
        "kind": 5,
        "importPath": "A2SL.test",
        "description": "A2SL.test",
        "peekOfCode": "quit_flag = False\nwhile not quit_flag:\n    success, img = cap.read()\n    if not success:\n        print(\"Failed to read frame.\")\n        continue  # Continue to the next iteration of the loop if frame reading fails\n    imgOutput = np.zeros((screen_height, screen_width, 3), np.uint8)  # Create black background\n    imgOutput[0:screen_height, 0:screen_width] = img  # Place original frame on black background\n    hands, img = detector.findHands(img)\n    # Check if hands are detected",
        "detail": "A2SL.test",
        "documentation": {}
    },
    {
        "label": "urlpatterns",
        "kind": 5,
        "importPath": "A2SL.urls",
        "description": "A2SL.urls",
        "peekOfCode": "urlpatterns = [\n    path('admin/', admin.site.urls),\n    path('about/',views.about_view,name='about'),\n    path('contact/',views.contact_view,name='contact'),\n    path('login/',views.login_view,name='login'),\n    path('logout/',views.logout_view,name='logout'),\n    path('signup/',views.signup_view,name='signup'),\n    path('animation/',views.animation_view,name='animation'),\n    path('handgestures/', views.handgestures_view, name='handgestures'),\n    path('spanish/',views.spanish_view,name='spanish'),",
        "detail": "A2SL.urls",
        "documentation": {}
    },
    {
        "label": "home_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def home_view(request):\n\treturn render(request,'home.html')\ndef about_view(request):\n\treturn render(request,'about.html')\ndef spanish_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "about_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def about_view(request):\n\treturn render(request,'about.html')\ndef spanish_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "spanish_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def spanish_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "korean_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def korean_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "french_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def french_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "arabic_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def arabic_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "hindi_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def hindi_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "telugu_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def telugu_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "urdu_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def urdu_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "german_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def german_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "japanese_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def japanese_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "contact_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def contact_view(request):\n\treturn render(request,'contact.html')\n@login_required(login_url=\"login\")\ndef animation_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('sen')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "animation_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def animation_view(request):\n\tif request.method == 'POST':\n\t\ttext = request.POST.get('sen')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "handgestures_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def handgestures_view(request):\n    if request.method == 'GET':\n        return render(request, 'handgestures.html')\n    elif request.method == 'POST':\n        if 'start_detection' in request.POST:\n            subprocess.Popen([\"python\", \"C://Users/Mohammed/audio2/A2SL/test.py\"])\n            return JsonResponse({'message': 'Real-time detection started. Click on CLOSE to exit'})\n        elif 'stop_detection' in request.POST:\n            # Add code to stop the real-time detection process if needed\n            return JsonResponse({'message': 'Real-time detection stopped'})",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "signup_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def signup_view(request):\n\tif request.method == 'POST':\n\t\tform = UserCreationForm(request.POST)\n\t\tif form.is_valid():\n\t\t\tuser = form.save()\n\t\t\tlogin(request,user)\n\t\t\t# log the user in\n\t\t\treturn redirect('animation')\n\telse:\n\t\tform = UserCreationForm()",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "login_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def login_view(request):\n\tif request.method == 'POST':\n\t\tform = AuthenticationForm(data=request.POST)\n\t\tif form.is_valid():\n\t\t\t#log in user\n\t\t\tuser = form.get_user()\n\t\t\tlogin(request,user)\n\t\t\tif 'next' in request.POST:\n\t\t\t\treturn redirect(request.POST.get('next'))\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "logout_view",
        "kind": 2,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "def logout_view(request):\n\tlogout(request)\n\treturn redirect(\"home\")",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='es',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'spanish.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'spanish.html')\ndef korean_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='ko',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'korean.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'korean.html')\ndef french_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='fr',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'french.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'french.html')\ndef arabic_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='ar',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'arabic.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'arabic.html')\ndef hindi_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='hi',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'hindi.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'hindi.html')\ndef telugu_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='te',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'telugu.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'telugu.html')\ndef urdu_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='ur',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'urdu.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'urdu.html')\ndef german_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='de',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'german.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'german.html')\ndef japanese_view(request):\n\tif request.method == 'POST':",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('ses')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\tword9=text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\tword9=text\n\t\ta=TextBlob(word9)\n\t\tb=a.translate(from_lang='ja',to ='en')\n\t\tb=b.split()\n\t\treturn render(request,'japanese.html',{'words':b,'text':text})\n\telse:\n\t\treturn render(request,'japanese.html')\ndef contact_view(request):\n\treturn render(request,'contact.html')",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttext",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttext = request.POST.get('sen')\n\t\t#tokenizing the sentence\n\t\ttext.lower()\n\t\t#tokenizing the sentence\n\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = word_tokenize(text)\n\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttagged",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttagged = nltk.pos_tag(words)\n\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense = {}\n\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"future\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"past\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])\n\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\ttense[\"present_continuous\"]",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\ttense[\"present_continuous\"] = len([word for word in tagged if word[1] in [\"VBG\"]])\n\t\t#stopwords that will be removed\n\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tstop_words",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tstop_words = set(['wasn', 'wouldn', 'has', 'that', 'does', 'shouldn', 'youve', 'off', 'for', 'didn',\n                          'aint', 'haven', 'werent', 'are', 'arent', 'shes', 'wasnt', 'its', 'havent', 'wouldn',\n                          'dont', 'youve', 'doesnt', 'hadnt', 'is', 'thatll', 'shouldve', 'then', 'the', 'mustn',\n                          'nor', 'as', 'its', 'needn', 'am', 'have', 'hasn', 'arent', 'youll', 'couldnt', 'youre',\n                          'mustnt', 'didn', 'doesnt', 'll', 'an', 'hadn', 'whom', 'hasnt', 'itself', 'needn',\n                          'shant', 'isnt', 'been', 'such', 'shan', 'aren', 'being', 'were', 'did', 'having',\n                          'might', 've', 'wont'])\n\t\t#removing stopwords and applying lemmatizing nlp process to words\n\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tlr",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tlr = WordNetLemmatizer()\n\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w,p in zip(words,tagged):\n\t\t\tif w not in stop_words:\n\t\t\t\tif p[1]=='VBG' or p[1]=='VBD' or p[1]=='VBZ' or p[1]=='VBN' or p[1]=='NN':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='v'))\n\t\t\t\telif p[1]=='JJ' or p[1]=='JJR' or p[1]=='JJS'or p[1]=='RBR' or p[1]=='RBS':\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w,pos='a'))\n\t\t\t\telse:\n\t\t\t\t\tfiltered_text.append(lr.lemmatize(w))\n\t\t#adding the specific word to specify tense",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\ttemp=[]\n\t\tfor w in words:\n\t\t\tif w=='I':\n\t\t\t\ttemp.append('Me')\n\t\t\telse:\n\t\t\t\ttemp.append(w)\n\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = temp\n\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tprobable_tense",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tprobable_tense = max(tense,key=tense.get)\n\t\tif probable_tense == \"past\" and tense[\"past\"]>=1:\n\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = [\"Before\"]\n\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\ttemp = temp + words\n\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\twords = temp\n\t\telif probable_tense == \"future\" and tense[\"future\"]>=1:\n\t\t\tif \"Will\" not in words:\n\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = [\"Will\"]\n\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\ttemp = temp + words\n\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\t\twords = temp\n\t\t\telse:\n\t\t\t\tpass\n\t\telif probable_tense == \"present\":\n\t\t\tif tense[\"present_continuous\"]>=1:\n\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = [\"Now\"]\n\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\ttemp",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\ttemp = temp + words\n\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\t\twords = temp\n\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tfiltered_text",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tfiltered_text = []\n\t\tfor w in words:\n\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tpath",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tpath = w + \".mp4\"\n\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tf",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tf = finders.find(path)\n\t\t\t#splitting the word if its animation is not present in database\n\t\t\tif not f:\n\t\t\t\tfor c in w:\n\t\t\t\t\tfiltered_text.append(c)\n\t\t\t#otherwise animation of word\n\t\t\telse:\n\t\t\t\tfiltered_text.append(w)\n\t\twords = filtered_text\n\t\treturn render(request,'animation.html',{'words':words,'text':text})",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\twords",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\twords = filtered_text\n\t\treturn render(request,'animation.html',{'words':words,'text':text})\n\telse:\n\t\treturn render(request,'animation.html')\n@login_required(login_url=\"login\")\ndef handgestures_view(request):\n    if request.method == 'GET':\n        return render(request, 'handgestures.html')\n    elif request.method == 'POST':\n        if 'start_detection' in request.POST:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tform",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tform = UserCreationForm(request.POST)\n\t\tif form.is_valid():\n\t\t\tuser = form.save()\n\t\t\tlogin(request,user)\n\t\t\t# log the user in\n\t\t\treturn redirect('animation')\n\telse:\n\t\tform = UserCreationForm()\n\treturn render(request,'signup.html',{'form':form})\ndef login_view(request):",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tuser",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tuser = form.save()\n\t\t\tlogin(request,user)\n\t\t\t# log the user in\n\t\t\treturn redirect('animation')\n\telse:\n\t\tform = UserCreationForm()\n\treturn render(request,'signup.html',{'form':form})\ndef login_view(request):\n\tif request.method == 'POST':\n\t\tform = AuthenticationForm(data=request.POST)",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tform",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tform = UserCreationForm()\n\treturn render(request,'signup.html',{'form':form})\ndef login_view(request):\n\tif request.method == 'POST':\n\t\tform = AuthenticationForm(data=request.POST)\n\t\tif form.is_valid():\n\t\t\t#log in user\n\t\t\tuser = form.get_user()\n\t\t\tlogin(request,user)\n\t\t\tif 'next' in request.POST:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tform",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tform = AuthenticationForm(data=request.POST)\n\t\tif form.is_valid():\n\t\t\t#log in user\n\t\t\tuser = form.get_user()\n\t\t\tlogin(request,user)\n\t\t\tif 'next' in request.POST:\n\t\t\t\treturn redirect(request.POST.get('next'))\n\t\t\telse:\n\t\t\t\treturn redirect('animation')\n\telse:",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\t\tuser",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\t\tuser = form.get_user()\n\t\t\tlogin(request,user)\n\t\t\tif 'next' in request.POST:\n\t\t\t\treturn redirect(request.POST.get('next'))\n\t\t\telse:\n\t\t\t\treturn redirect('animation')\n\telse:\n\t\tform = AuthenticationForm()\n\treturn render(request,'login.html',{'form':form})\ndef logout_view(request):",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "\t\tform",
        "kind": 5,
        "importPath": "A2SL.views",
        "description": "A2SL.views",
        "peekOfCode": "\t\tform = AuthenticationForm()\n\treturn render(request,'login.html',{'form':form})\ndef logout_view(request):\n\tlogout(request)\n\treturn redirect(\"home\")",
        "detail": "A2SL.views",
        "documentation": {}
    },
    {
        "label": "application",
        "kind": 5,
        "importPath": "A2SL.wsgi",
        "description": "A2SL.wsgi",
        "peekOfCode": "application = get_wsgi_application()",
        "detail": "A2SL.wsgi",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "manage",
        "description": "manage",
        "peekOfCode": "def main():\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'A2SL.settings')\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            \"Couldn't import Django. Are you sure it's installed and \"\n            \"available on your PYTHONPATH environment variable? Did you \"\n            \"forget to activate a virtual environment?\"\n        ) from exc",
        "detail": "manage",
        "documentation": {}
    }
]